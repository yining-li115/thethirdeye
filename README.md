# ğŸ§­ The Third Eye  
_A Voice-Guided AI Navigation System for Blind & Low-Vision Shoppers_

---

## ğŸ—ï¸ System Overview  
The Third Eye works as an **end-to-end multimodal pipeline** connecting:

- Speech understanding  
- Vision perception  
- Depth / 3D reasoning  
- Navigation instructions  
- Multi-agent orchestration  

Everything runs automatically as the user talks and moves.

---

## ğŸ”§ How It Works (Pipeline)

### **1. Voice Command â†’ Intent Understanding**
User speaks: â€œWhere is the milk?â€

Our **STT agent** converts audio â†’ text.  
Then the **Intent agent** extracts the object keyword (e.g., â€œmilkâ€).

### **2. Real-Time Visual Perception**
The camera continuously captures frames.  
The **VLM agent** analyzes them to:

- detect the target item  
- determine confidence  
- return bounding box and location  

### **3. Multi-Agent Decision System**
If object is found â†’ trigger navigation  
If not â†’ stay silent and wait for new frames  
(avoiding noisy or disruptive feedback)

### **4. 3D Spatial Localization**
Once detected, the 3D reconstruction module estimates:

- the userâ€™s camera pose  
- the objectâ€™s approximate 3D coordinates  

This answers:
**Left or right? How far? What angle?**

### **5. Relative Position Calculation**
We compute:

- direction (e.g., â€œ30Â° rightâ€)  
- distance (e.g., â€œ2.1 m awayâ€)  
- forward / left / right orientation  

### **6. Audio Navigation Output**
Finally, the **Navigation agent** generates natural, clear instructions:

- â€œThe apples are two meters ahead on your right.â€  
- â€œMove slightly left.â€  
- â€œReach forward


